{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8241085,"sourceType":"datasetVersion","datasetId":4888630}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T06:17:11.975474Z","iopub.execute_input":"2024-04-29T06:17:11.976432Z","iopub.status.idle":"2024-04-29T06:17:12.036013Z","shell.execute_reply.started":"2024-04-29T06:17:11.976395Z","shell.execute_reply":"2024-04-29T06:17:12.035023Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_data(image_dir, mask_dir, batch_size):\n    image_filenames = [filename for filename in os.listdir(image_dir) if filename.endswith('.jpg')]\n    num_images = len(image_filenames)\n    \n    for i in range(0, num_images, batch_size):\n        print(\"Loading batch\", (i // batch_size) + 1, \"of\", (num_images // batch_size) + 1)\n        images_batch = []\n        masks_batch = []\n        batch_filenames = image_filenames[i:i+batch_size]\n        \n        for filename in batch_filenames:\n            # Load image\n            image_path = os.path.join(image_dir, filename)\n            image = cv2.imread(image_path)\n            image = cv2.resize(image, (2, 256))  # Resize image if necessary\n            images_batch.append(image)\n            \n            # Load corresponding mask\n            mask_filename = filename[:-4] + '_segmentation.png'\n            mask_path = os.path.join(mask_dir, mask_filename)\n            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n            mask = cv2.resize(mask, (256, 256))  # Resize mask if necessary\n            masks_batch.append(mask)\n        \n        yield (np.array(images_batch)/255,(np.array(masks_batch)/255).astype(int))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:17:12.332454Z","iopub.execute_input":"2024-04-29T06:17:12.333366Z","iopub.status.idle":"2024-04-29T06:17:12.343475Z","shell.execute_reply.started":"2024-04-29T06:17:12.333333Z","shell.execute_reply":"2024-04-29T06:17:12.342374Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_image(image_dir, mask_dir, batch_size):\n    data_generator = load_data(image_dir, mask_dir, batch_size)\n    images_all = []\n    masks_all = []\n    num_images = len([filename for filename in os.listdir(image_dir) if filename.endswith('.jpg')])\n    num_batches = (num_images + batch_size - 1) // batch_size\n    \n    for _ in range(num_batches):\n        data = next(data_generator)\n        images, masks = data\n        images_all.append(images)\n        masks_all.append(masks)\n    \n    images_all = np.concatenate(images_all, axis=0)\n    masks_all = np.concatenate(masks_all, axis=0)\n    print(\"Total images shape:\", images_all.shape)\n    print(\"Total masks shape:\", masks_all.shape)\n    return images_all, masks_all","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:17:12.906238Z","iopub.execute_input":"2024-04-29T06:17:12.906986Z","iopub.status.idle":"2024-04-29T06:17:12.913845Z","shell.execute_reply.started":"2024-04-29T06:17:12.906952Z","shell.execute_reply":"2024-04-29T06:17:12.912988Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_images, train_masks = load_image('/kaggle/input/isic2018/ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input','/kaggle/input/isic2018/ISIC2018_Task1_Training_GroundTruth/ISIC2018_Task1_Training_GroundTruth',128)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:17:13.403132Z","iopub.execute_input":"2024-04-29T06:17:13.403882Z","iopub.status.idle":"2024-04-29T06:26:56.193001Z","shell.execute_reply.started":"2024-04-29T06:17:13.403847Z","shell.execute_reply":"2024-04-29T06:26:56.191932Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Loading batch 1 of 21\nLoading batch 2 of 21\nLoading batch 3 of 21\nLoading batch 4 of 21\nLoading batch 5 of 21\nLoading batch 6 of 21\nLoading batch 7 of 21\nLoading batch 8 of 21\nLoading batch 9 of 21\nLoading batch 10 of 21\nLoading batch 11 of 21\nLoading batch 12 of 21\nLoading batch 13 of 21\nLoading batch 14 of 21\nLoading batch 15 of 21\nLoading batch 16 of 21\nLoading batch 17 of 21\nLoading batch 18 of 21\nLoading batch 19 of 21\nLoading batch 20 of 21\nLoading batch 21 of 21\nTotal images shape: (2594, 256, 256, 3)\nTotal masks shape: (2594, 256, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_images, test_masks = load_image('/kaggle/input/isic2018/ISIC2018_Task1-2_Test_Input/ISIC2018_Task1-2_Test_Input','/kaggle/input/isic2018/ISIC2018_Task1_Test_GroundTruth/ISIC2018_Task1_Test_GroundTruth',64)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:26:56.194779Z","iopub.execute_input":"2024-04-29T06:26:56.195106Z","iopub.status.idle":"2024-04-29T06:29:11.702968Z","shell.execute_reply.started":"2024-04-29T06:26:56.195079Z","shell.execute_reply":"2024-04-29T06:29:11.701957Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Loading batch 1 of 16\nLoading batch 2 of 16\nLoading batch 3 of 16\nLoading batch 4 of 16\nLoading batch 5 of 16\nLoading batch 6 of 16\nLoading batch 7 of 16\nLoading batch 8 of 16\nLoading batch 9 of 16\nLoading batch 10 of 16\nLoading batch 11 of 16\nLoading batch 12 of 16\nLoading batch 13 of 16\nLoading batch 14 of 16\nLoading batch 15 of 16\nLoading batch 16 of 16\nTotal images shape: (1000, 256, 256, 3)\nTotal masks shape: (1000, 256, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_masks[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:29:11.704396Z","iopub.execute_input":"2024-04-29T06:29:11.704792Z","iopub.status.idle":"2024-04-29T06:29:11.713216Z","shell.execute_reply.started":"2024-04-29T06:29:11.704757Z","shell.execute_reply":"2024-04-29T06:29:11.712348Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"val_images, val_masks = load_image('/kaggle/input/isic2018/ISIC2018_Task1-2_Validation_Input/ISIC2018_Task1-2_Validation_Input','/kaggle/input/isic2018/ISIC2018_Task1_Validation_GroundTruth/ISIC2018_Task1_Validation_GroundTruth',64)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:29:11.715235Z","iopub.execute_input":"2024-04-29T06:29:11.715603Z","iopub.status.idle":"2024-04-29T06:29:24.612217Z","shell.execute_reply.started":"2024-04-29T06:29:11.715538Z","shell.execute_reply":"2024-04-29T06:29:24.611294Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Loading batch 1 of 2\nLoading batch 2 of 2\nTotal images shape: (100, 256, 256, 3)\nTotal masks shape: (100, 256, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"def unet_model(input_shape, learning_rate=1e-4, dropout_rate=0.2, weight_decay=1e-5):\n    inputs = Input(input_shape)\n    \n    # Encoder\n    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    pool1 = Dropout(dropout_rate)(pool1)\n    \n    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    pool2 = Dropout(dropout_rate)(pool2)\n    \n    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    pool3 = Dropout(dropout_rate)(pool3)\n    \n    # Bottleneck\n    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv4)\n    conv4 = Dropout(dropout_rate)(conv4)\n    \n    # Decoder\n    up5 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4)\n    up5 = concatenate([up5, conv3], axis=3)\n    conv5 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(up5)\n    conv5 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv5)\n    conv5 = Dropout(dropout_rate)(conv5)\n    \n    up6 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5)\n    up6 = concatenate([up6, conv2], axis=3)\n    conv6 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(up6)\n    conv6 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv6)\n    conv6 = Dropout(dropout_rate)(conv6)\n    \n    up7 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv6)\n    up7 = concatenate([up7, conv1], axis=3)\n    conv7 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(up7)\n    conv7 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv7)\n    \n    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compile model\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:06:51.819828Z","iopub.execute_input":"2024-04-29T04:06:51.820536Z","iopub.status.idle":"2024-04-29T04:06:51.837272Z","shell.execute_reply.started":"2024-04-29T04:06:51.820505Z","shell.execute_reply":"2024-04-29T04:06:51.836401Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score, confusion_matrix\n\ndef threshold_jaccard_index(predicted_masks, ground_truth_masks, threshold=0.65):\n    scores = []\n    for i in range(len(predicted_masks)):\n        # Binarize the predicted mask using a threshold\n        #binary_predicted_mask = (predicted_masks[i] > 0.5).astype(int) * 255\n        \n        # Compute the Jaccard index between the binary predicted mask and ground truth mask\n        jaccard_index = jaccard_score(ground_truth_masks[i].flatten(), predicted_masks[i].flatten(),labels=[0, 1], average=\"binary\")\n        overall_jaccard_index = np.mean(jaccard_index)  # Compute the overall Jaccard index for the entire mask\n        if overall_jaccard_index < threshold:\n            scores.append(0)\n        else:\n            scores.append(overall_jaccard_index)\n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:00:07.545352Z","iopub.execute_input":"2024-04-29T07:00:07.546121Z","iopub.status.idle":"2024-04-29T07:00:07.552951Z","shell.execute_reply.started":"2024-04-29T07:00:07.546086Z","shell.execute_reply":"2024-04-29T07:00:07.551924Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def jaccard_index(predicted_mask, ground_truth_mask):\n    return jaccard_score(ground_truth_mask.flatten(), predicted_mask.flatten())","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:19:09.675673Z","iopub.execute_input":"2024-04-29T03:19:09.676882Z","iopub.status.idle":"2024-04-29T03:19:09.681512Z","shell.execute_reply.started":"2024-04-29T03:19:09.676837Z","shell.execute_reply":"2024-04-29T03:19:09.680523Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0000005\ndropout_rate = 0.2\nweight_decay = 1e-5\n\n\nmodel = unet_model(input_shape=(256, 256, 3), learning_rate=learning_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n\n# Callbacks\ncheckpoint = ModelCheckpoint(\"unet_model.keras\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='min', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, min_lr=0.0005, verbose=1, mode='min')\n\n# Train the model\nhistory = model.fit(train_images, train_masks, batch_size=16, epochs=100, validation_split = 0.2, callbacks=[checkpoint, early_stopping, reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T04:42:59.921119Z","iopub.execute_input":"2024-04-29T04:42:59.921486Z","iopub.status.idle":"2024-04-29T05:35:46.049605Z","shell.execute_reply.started":"2024-04-29T04:42:59.921455Z","shell.execute_reply":"2024-04-29T05:35:46.048406Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - loss: 0.7121\nEpoch 1: val_loss improved from inf to 0.69623, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 428ms/step - loss: 0.7120 - val_loss: 0.6962 - learning_rate: 5.0000e-07\nEpoch 2/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.6904\nEpoch 2: val_loss improved from 0.69623 to 0.67639, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.6904 - val_loss: 0.6764 - learning_rate: 5.0000e-07\nEpoch 3/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.6696\nEpoch 3: val_loss improved from 0.67639 to 0.65617, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.6695 - val_loss: 0.6562 - learning_rate: 5.0000e-07\nEpoch 4/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.6473\nEpoch 4: val_loss improved from 0.65617 to 0.63137, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.6472 - val_loss: 0.6314 - learning_rate: 5.0000e-07\nEpoch 5/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.6190\nEpoch 5: val_loss improved from 0.63137 to 0.60070, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.6189 - val_loss: 0.6007 - learning_rate: 5.0000e-07\nEpoch 6/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.5772\nEpoch 6: val_loss improved from 0.60070 to 0.56240, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5771 - val_loss: 0.5624 - learning_rate: 5.0000e-07\nEpoch 7/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5352\nEpoch 7: val_loss improved from 0.56240 to 0.53846, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5351 - val_loss: 0.5385 - learning_rate: 5.0000e-07\nEpoch 8/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5276\nEpoch 8: val_loss improved from 0.53846 to 0.53367, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.5276 - val_loss: 0.5337 - learning_rate: 5.0000e-07\nEpoch 9/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5167\nEpoch 9: val_loss improved from 0.53367 to 0.53163, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5167 - val_loss: 0.5316 - learning_rate: 5.0000e-07\nEpoch 10/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5098\nEpoch 10: val_loss improved from 0.53163 to 0.53000, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.5098 - val_loss: 0.5300 - learning_rate: 5.0000e-07\nEpoch 11/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5132\nEpoch 11: val_loss improved from 0.53000 to 0.52828, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5132 - val_loss: 0.5283 - learning_rate: 5.0000e-07\nEpoch 12/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5050\nEpoch 12: val_loss improved from 0.52828 to 0.52652, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5050 - val_loss: 0.5265 - learning_rate: 5.0000e-07\nEpoch 13/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5040\nEpoch 13: val_loss improved from 0.52652 to 0.52473, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5040 - val_loss: 0.5247 - learning_rate: 5.0000e-07\nEpoch 14/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5085\nEpoch 14: val_loss improved from 0.52473 to 0.52289, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5085 - val_loss: 0.5229 - learning_rate: 5.0000e-07\nEpoch 15/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5177\nEpoch 15: val_loss improved from 0.52289 to 0.52111, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.5176 - val_loss: 0.5211 - learning_rate: 5.0000e-07\nEpoch 16/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5087\nEpoch 16: val_loss improved from 0.52111 to 0.51911, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5086 - val_loss: 0.5191 - learning_rate: 5.0000e-07\nEpoch 17/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5023\nEpoch 17: val_loss improved from 0.51911 to 0.51704, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.5022 - val_loss: 0.5170 - learning_rate: 5.0000e-07\nEpoch 18/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4930\nEpoch 18: val_loss improved from 0.51704 to 0.51504, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4930 - val_loss: 0.5150 - learning_rate: 5.0000e-07\nEpoch 19/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.5048\nEpoch 19: val_loss improved from 0.51504 to 0.51258, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.5048 - val_loss: 0.5126 - learning_rate: 5.0000e-07\nEpoch 20/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4874\nEpoch 20: val_loss improved from 0.51258 to 0.51003, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4874 - val_loss: 0.5100 - learning_rate: 5.0000e-07\nEpoch 21/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4866\nEpoch 21: val_loss improved from 0.51003 to 0.50759, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4866 - val_loss: 0.5076 - learning_rate: 5.0000e-07\nEpoch 22/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4834\nEpoch 22: val_loss improved from 0.50759 to 0.50428, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4835 - val_loss: 0.5043 - learning_rate: 5.0000e-07\nEpoch 23/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4838\nEpoch 23: val_loss improved from 0.50428 to 0.50072, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4838 - val_loss: 0.5007 - learning_rate: 5.0000e-07\nEpoch 24/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4814\nEpoch 24: val_loss improved from 0.50072 to 0.49689, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4814 - val_loss: 0.4969 - learning_rate: 5.0000e-07\nEpoch 25/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4824\nEpoch 25: val_loss improved from 0.49689 to 0.49196, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.4824 - val_loss: 0.4920 - learning_rate: 5.0000e-07\nEpoch 26/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4657\nEpoch 26: val_loss improved from 0.49196 to 0.48869, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4657 - val_loss: 0.4887 - learning_rate: 5.0000e-07\nEpoch 27/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4677\nEpoch 27: val_loss improved from 0.48869 to 0.48398, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4677 - val_loss: 0.4840 - learning_rate: 5.0000e-07\nEpoch 28/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4587\nEpoch 28: val_loss improved from 0.48398 to 0.47905, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4587 - val_loss: 0.4790 - learning_rate: 5.0000e-07\nEpoch 29/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4556\nEpoch 29: val_loss improved from 0.47905 to 0.47662, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4556 - val_loss: 0.4766 - learning_rate: 5.0000e-07\nEpoch 30/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4505\nEpoch 30: val_loss improved from 0.47662 to 0.47239, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4505 - val_loss: 0.4724 - learning_rate: 5.0000e-07\nEpoch 31/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4558\nEpoch 31: val_loss improved from 0.47239 to 0.47009, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.4557 - val_loss: 0.4701 - learning_rate: 5.0000e-07\nEpoch 32/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4487\nEpoch 32: val_loss improved from 0.47009 to 0.46803, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4487 - val_loss: 0.4680 - learning_rate: 5.0000e-07\nEpoch 33/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4424\nEpoch 33: val_loss improved from 0.46803 to 0.46614, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4424 - val_loss: 0.4661 - learning_rate: 5.0000e-07\nEpoch 34/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4412\nEpoch 34: val_loss improved from 0.46614 to 0.46394, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4412 - val_loss: 0.4639 - learning_rate: 5.0000e-07\nEpoch 35/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4384\nEpoch 35: val_loss improved from 0.46394 to 0.46283, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4384 - val_loss: 0.4628 - learning_rate: 5.0000e-07\nEpoch 36/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4370\nEpoch 36: val_loss improved from 0.46283 to 0.46108, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4370 - val_loss: 0.4611 - learning_rate: 5.0000e-07\nEpoch 37/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4358\nEpoch 37: val_loss improved from 0.46108 to 0.46078, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4358 - val_loss: 0.4608 - learning_rate: 5.0000e-07\nEpoch 38/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4325\nEpoch 38: val_loss improved from 0.46078 to 0.45799, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4325 - val_loss: 0.4580 - learning_rate: 5.0000e-07\nEpoch 39/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4310\nEpoch 39: val_loss improved from 0.45799 to 0.45727, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4310 - val_loss: 0.4573 - learning_rate: 5.0000e-07\nEpoch 40/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4329\nEpoch 40: val_loss improved from 0.45727 to 0.45669, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4329 - val_loss: 0.4567 - learning_rate: 5.0000e-07\nEpoch 41/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4205\nEpoch 41: val_loss improved from 0.45669 to 0.45532, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4206 - val_loss: 0.4553 - learning_rate: 5.0000e-07\nEpoch 42/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4227\nEpoch 42: val_loss improved from 0.45532 to 0.45398, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4228 - val_loss: 0.4540 - learning_rate: 5.0000e-07\nEpoch 43/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4252\nEpoch 43: val_loss improved from 0.45398 to 0.45261, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4253 - val_loss: 0.4526 - learning_rate: 5.0000e-07\nEpoch 44/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4223\nEpoch 44: val_loss improved from 0.45261 to 0.44945, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4223 - val_loss: 0.4494 - learning_rate: 5.0000e-07\nEpoch 45/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4234\nEpoch 45: val_loss improved from 0.44945 to 0.44799, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4234 - val_loss: 0.4480 - learning_rate: 5.0000e-07\nEpoch 46/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4217\nEpoch 46: val_loss improved from 0.44799 to 0.44627, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 304ms/step - loss: 0.4217 - val_loss: 0.4463 - learning_rate: 5.0000e-07\nEpoch 47/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4254\nEpoch 47: val_loss improved from 0.44627 to 0.44425, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.4253 - val_loss: 0.4442 - learning_rate: 5.0000e-07\nEpoch 48/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4225\nEpoch 48: val_loss improved from 0.44425 to 0.44259, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.4225 - val_loss: 0.4426 - learning_rate: 5.0000e-07\nEpoch 49/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4177\nEpoch 49: val_loss improved from 0.44259 to 0.44025, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4177 - val_loss: 0.4403 - learning_rate: 5.0000e-07\nEpoch 50/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4239\nEpoch 50: val_loss improved from 0.44025 to 0.43847, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.4239 - val_loss: 0.4385 - learning_rate: 5.0000e-07\nEpoch 51/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4048\nEpoch 51: val_loss improved from 0.43847 to 0.43579, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4048 - val_loss: 0.4358 - learning_rate: 5.0000e-07\nEpoch 52/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4153\nEpoch 52: val_loss improved from 0.43579 to 0.43341, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4152 - val_loss: 0.4334 - learning_rate: 5.0000e-07\nEpoch 53/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.4016\nEpoch 53: val_loss improved from 0.43341 to 0.43167, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.4016 - val_loss: 0.4317 - learning_rate: 5.0000e-07\nEpoch 54/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.4082\nEpoch 54: val_loss improved from 0.43167 to 0.42947, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.4081 - val_loss: 0.4295 - learning_rate: 5.0000e-07\nEpoch 55/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3929\nEpoch 55: val_loss improved from 0.42947 to 0.42761, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.3930 - val_loss: 0.4276 - learning_rate: 5.0000e-07\nEpoch 56/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3965\nEpoch 56: val_loss improved from 0.42761 to 0.42557, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.3965 - val_loss: 0.4256 - learning_rate: 5.0000e-07\nEpoch 57/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3905\nEpoch 57: val_loss improved from 0.42557 to 0.42107, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 304ms/step - loss: 0.3906 - val_loss: 0.4211 - learning_rate: 5.0000e-07\nEpoch 58/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3916\nEpoch 58: val_loss improved from 0.42107 to 0.41831, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 303ms/step - loss: 0.3916 - val_loss: 0.4183 - learning_rate: 5.0000e-07\nEpoch 59/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3888\nEpoch 59: val_loss improved from 0.41831 to 0.41779, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.3889 - val_loss: 0.4178 - learning_rate: 5.0000e-07\nEpoch 60/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3951\nEpoch 60: val_loss did not improve from 0.41779\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 300ms/step - loss: 0.3951 - val_loss: 0.4190 - learning_rate: 5.0000e-07\nEpoch 61/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3888\nEpoch 61: val_loss did not improve from 0.41779\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3888 - val_loss: 0.4189 - learning_rate: 5.0000e-07\nEpoch 62/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3755\nEpoch 62: val_loss improved from 0.41779 to 0.41367, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.3756 - val_loss: 0.4137 - learning_rate: 5.0000e-07\nEpoch 63/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3848\nEpoch 63: val_loss improved from 0.41367 to 0.41328, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - loss: 0.3848 - val_loss: 0.4133 - learning_rate: 5.0000e-07\nEpoch 64/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3809\nEpoch 64: val_loss did not improve from 0.41328\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3809 - val_loss: 0.4188 - learning_rate: 5.0000e-07\nEpoch 65/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3840\nEpoch 65: val_loss did not improve from 0.41328\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3840 - val_loss: 0.4151 - learning_rate: 5.0000e-07\nEpoch 66/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3853\nEpoch 66: val_loss did not improve from 0.41328\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 300ms/step - loss: 0.3853 - val_loss: 0.4137 - learning_rate: 5.0000e-07\nEpoch 67/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3824\nEpoch 67: val_loss did not improve from 0.41328\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 300ms/step - loss: 0.3824 - val_loss: 0.4165 - learning_rate: 5.0000e-07\nEpoch 68/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3731\nEpoch 68: val_loss did not improve from 0.41328\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3732 - val_loss: 0.4167 - learning_rate: 5.0000e-07\nEpoch 69/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3792\nEpoch 69: val_loss did not improve from 0.41328\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3792 - val_loss: 0.4180 - learning_rate: 5.0000e-07\nEpoch 70/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3819\nEpoch 70: val_loss improved from 0.41328 to 0.41119, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.3819 - val_loss: 0.4112 - learning_rate: 5.0000e-07\nEpoch 71/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3854\nEpoch 71: val_loss did not improve from 0.41119\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3854 - val_loss: 0.4167 - learning_rate: 5.0000e-07\nEpoch 72/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3672\nEpoch 72: val_loss improved from 0.41119 to 0.41039, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - loss: 0.3673 - val_loss: 0.4104 - learning_rate: 5.0000e-07\nEpoch 73/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3873\nEpoch 73: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3872 - val_loss: 0.4160 - learning_rate: 5.0000e-07\nEpoch 74/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3813\nEpoch 74: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3813 - val_loss: 0.4185 - learning_rate: 5.0000e-07\nEpoch 75/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3759\nEpoch 75: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3759 - val_loss: 0.4247 - learning_rate: 5.0000e-07\nEpoch 76/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3758\nEpoch 76: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3758 - val_loss: 0.4213 - learning_rate: 5.0000e-07\nEpoch 77/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3719\nEpoch 77: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - loss: 0.3719 - val_loss: 0.4113 - learning_rate: 5.0000e-07\nEpoch 78/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.3718\nEpoch 78: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 300ms/step - loss: 0.3718 - val_loss: 0.4158 - learning_rate: 5.0000e-07\nEpoch 79/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.3763\nEpoch 79: val_loss did not improve from 0.41039\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 300ms/step - loss: 0.3763 - val_loss: 0.4205 - learning_rate: 5.0000e-07\nEpoch 79: early stopping\nRestoring model weights from the end of the best epoch: 72.\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_masks = model.predict(test_images)\npredicted_masks = (predicted_masks > 0.5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:35:56.513234Z","iopub.execute_input":"2024-04-29T05:35:56.513661Z","iopub.status.idle":"2024-04-29T05:36:05.447182Z","shell.execute_reply.started":"2024-04-29T05:35:56.513627Z","shell.execute_reply":"2024-04-29T05:36:05.446347Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 178ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"threshold_jaccard = threshold_jaccard_index(predicted_masks, test_masks)\nprint(\"Threshold Jaccard Index:\", threshold_jaccard)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:39:23.909845Z","iopub.execute_input":"2024-04-29T05:39:23.910601Z","iopub.status.idle":"2024-04-29T05:39:53.124012Z","shell.execute_reply.started":"2024-04-29T05:39:23.910532Z","shell.execute_reply":"2024-04-29T05:39:53.122950Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Threshold Jaccard Index: 0.3750459036925851\n","output_type":"stream"}]},{"cell_type":"code","source":"def custom_unet_model(input_shape, learning_rate=1e-4, dropout_rate=0.2, weight_decay=1e-5, num_filters=64, num_layers=4):\n    inputs = Input(input_shape)\n    \n    # Encoder\n    conv_layers = []\n    pool_layers = []\n    x = inputs\n    for _ in range(num_layers):\n        conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(x)\n        conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv)\n        pool = MaxPooling2D(pool_size=(2, 2))(conv)\n        pool = Dropout(dropout_rate)(pool)\n        conv_layers.append(conv)\n        pool_layers.append(pool)\n        x = pool\n    \n    # Bottleneck\n    conv = Conv2D(num_filters * 2, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(x)\n    conv = Conv2D(num_filters * 2, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv)\n    conv = Dropout(dropout_rate)(conv)\n    \n    # Decoder\n    for conv_layer, pool_layer in zip(reversed(conv_layers), reversed(pool_layers)):\n        up = Conv2DTranspose(num_filters, 2, strides=(2, 2), padding='same')(conv)\n        up = concatenate([up, conv_layer], axis=3)\n        conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(up)\n        conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_regularizer=l2(weight_decay))(conv)\n        conv = Dropout(dropout_rate)(conv)\n    \n    outputs = Conv2D(1, 1, activation='sigmoid')(conv)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compile model\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n    \n    return model\n\nmodel = custom_unet_model(input_shape=(256, 256, 3))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:29:24.613216Z","iopub.execute_input":"2024-04-29T06:29:24.613472Z","iopub.status.idle":"2024-04-29T06:29:25.292712Z","shell.execute_reply.started":"2024-04-29T06:29:24.613450Z","shell.execute_reply":"2024-04-29T06:29:25.291900Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\ndef build_model(hp):\n    model = custom_unet_model(\n        input_shape=(256, 256, 3),\n        learning_rate=hp.Float('learning_rate', 1e-6, 1e-2, sampling='log'),\n        dropout_rate=hp.Float('dropout_rate', 0.1, 0.5, step=0.1),\n        weight_decay=hp.Float('weight_decay', 1e-6, 1e-3, sampling='log'),\n        num_filters=hp.Choice('num_filters', [32, 64, 128]),\n        num_layers=hp.Int('num_layers', 3, 5)\n    )\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=10,\n    executions_per_trial=1,\n    directory='hyperparameter_tuning',\n    project_name='custom_unet'\n)\n\ntuner.search(train_images, train_masks, validation_split = 0.2, epochs=10, batch_size=16)\nbest_model = tuner.get_best_models(num_models=1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"Best Hyperparameters:\")\nprint(best_hyperparameters.values)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T06:29:25.293810Z","iopub.execute_input":"2024-04-29T06:29:25.294139Z","iopub.status.idle":"2024-04-29T06:58:06.579991Z","shell.execute_reply.started":"2024-04-29T06:29:25.294114Z","shell.execute_reply":"2024-04-29T06:58:06.575341Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Trial 10 Complete [00h 02m 47s]\nval_loss: 0.37028780579566956\n\nBest val_loss So Far: 0.37028780579566956\nTotal elapsed time: 01h 07m 37s\nBest Hyperparameters:\n{'learning_rate': 6.901714677917224e-05, 'dropout_rate': 0.2, 'weight_decay': 7.792822719960166e-05, 'num_filters': 32, 'num_layers': 4}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 94 variables. \n  trackable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_masks = best_model.predict(test_images)\npredicted_masks = (predicted_masks > 0.5).astype(int)\nthreshold_jaccard = threshold_jaccard_index(predicted_masks, test_masks)\nprint(\"Threshold Jaccard Index:\", threshold_jaccard)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:00:15.210946Z","iopub.execute_input":"2024-04-29T07:00:15.211619Z","iopub.status.idle":"2024-04-29T07:00:47.336528Z","shell.execute_reply.started":"2024-04-29T07:00:15.211584Z","shell.execute_reply":"2024-04-29T07:00:47.335290Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\nThreshold Jaccard Index: 0.4924629954628306\n","output_type":"stream"}]},{"cell_type":"code","source":"model = custom_unet_model(\n        input_shape=(256, 256, 3),\n        learning_rate=7e-05,\n        dropout_rate=0.2,\n        weight_decay=7.792822719960166e-05,\n        num_filters=32,\n        num_layers=5,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:57:32.816999Z","iopub.execute_input":"2024-04-29T07:57:32.817398Z","iopub.status.idle":"2024-04-29T07:57:33.053107Z","shell.execute_reply.started":"2024-04-29T07:57:32.817367Z","shell.execute_reply":"2024-04-29T07:57:33.052122Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"unet_model.keras\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0005, verbose=1, mode='min')\n\n# Train the model\nhistory = model.fit(train_images, train_masks, batch_size=16, epochs=100, validation_split = 0.2, callbacks=[checkpoint, early_stopping, reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:57:35.122680Z","iopub.execute_input":"2024-04-29T07:57:35.123757Z","iopub.status.idle":"2024-04-29T07:59:55.141403Z","shell.execute_reply.started":"2024-04-29T07:57:35.123718Z","shell.execute_reply":"2024-04-29T07:59:55.140511Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 0.5663\nEpoch 1: val_loss improved from inf to 0.47907, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 279ms/step - loss: 0.5659 - val_loss: 0.4791 - learning_rate: 7.0000e-05\nEpoch 2/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.4217\nEpoch 2: val_loss improved from 0.47907 to 0.46393, saving model to unet_model.keras\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 89ms/step - loss: 0.4216 - val_loss: 0.4639 - learning_rate: 7.0000e-05\nEpoch 3/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3651\nEpoch 3: val_loss did not improve from 0.46393\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - loss: 0.3650 - val_loss: 0.4856 - learning_rate: 7.0000e-05\nEpoch 4/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.3016\nEpoch 4: val_loss did not improve from 0.46393\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - loss: 1.3096 - val_loss: 3.6266 - learning_rate: 7.0000e-05\nEpoch 5/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.5775\nEpoch 5: val_loss did not improve from 0.46393\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - loss: 3.5764 - val_loss: 3.6250 - learning_rate: 7.0000e-05\nEpoch 6/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.5828\nEpoch 6: val_loss did not improve from 0.46393\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - loss: 3.5816 - val_loss: 3.6229 - learning_rate: 7.0000e-05\nEpoch 7/100\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.4163\nEpoch 7: val_loss did not improve from 0.46393\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - loss: 3.4164 - val_loss: 3.6207 - learning_rate: 7.0000e-05\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_masks = model.predict(test_images)\npredicted_masks = (predicted_masks > 0.5).astype(int)\nthreshold_jaccard = threshold_jaccard_index(predicted_masks, test_masks)\nprint(\"Threshold Jaccard Index:\", threshold_jaccard)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:17:06.269627Z","iopub.execute_input":"2024-04-29T07:17:06.270432Z","iopub.status.idle":"2024-04-29T07:17:42.372952Z","shell.execute_reply.started":"2024-04-29T07:17:06.270392Z","shell.execute_reply":"2024-04-29T07:17:42.371965Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step\nThreshold Jaccard Index: 0.5090880172650178\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}